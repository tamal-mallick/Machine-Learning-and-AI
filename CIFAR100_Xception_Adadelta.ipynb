{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR100_Xception_Adadelta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1agGD7uP493Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b01615e-dd5d-486a-9056-c4e922fdcf35"
      },
      "source": [
        "#https://medium.com/@kenneth.ca95/a-guide-to-transfer-learning-with-keras-using-Xception-a81a4a28084b\n",
        "# Running the version as 1.x is optional, without that first line it will run the last version of tensorflow for Colab.\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUEuZsobBTUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a560b3a0-9886-47db-d3b7-d80484bf8e35"
      },
      "source": [
        "# Load data\n",
        "# Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples \n",
        "# and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n",
        "# We can take advantage of the fact that these categories and a lot more are into the Imagenet collection.\n",
        "\n",
        "#tf.keras.datasets.cifar10.load_data()\n",
        "#tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "tf.keras.datasets.cifar100.load_data()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[[[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [195, 205, 193],\n",
              "           [212, 224, 204],\n",
              "           [182, 194, 167]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [254, 254, 254],\n",
              "           [254, 254, 254],\n",
              "           ...,\n",
              "           [170, 176, 150],\n",
              "           [161, 168, 130],\n",
              "           [146, 154, 113]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [254, 254, 254],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [189, 199, 169],\n",
              "           [166, 178, 130],\n",
              "           [121, 133,  87]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[148, 185,  79],\n",
              "           [142, 182,  57],\n",
              "           [140, 179,  60],\n",
              "           ...,\n",
              "           [ 30,  17,   1],\n",
              "           [ 65,  62,  15],\n",
              "           [ 76,  77,  20]],\n",
              "  \n",
              "          [[122, 157,  66],\n",
              "           [120, 155,  58],\n",
              "           [126, 160,  71],\n",
              "           ...,\n",
              "           [ 22,  16,   3],\n",
              "           [ 97, 112,  56],\n",
              "           [141, 161,  87]],\n",
              "  \n",
              "          [[ 87, 122,  41],\n",
              "           [ 88, 122,  39],\n",
              "           [101, 134,  56],\n",
              "           ...,\n",
              "           [ 34,  36,  10],\n",
              "           [105, 133,  59],\n",
              "           [138, 173,  79]]],\n",
              "  \n",
              "  \n",
              "         [[[255, 255, 255],\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           ...,\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           ...,\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           [255, 255, 255]]],\n",
              "  \n",
              "  \n",
              "         [[[250, 250, 248],\n",
              "           [248, 249, 243],\n",
              "           [247, 248, 239],\n",
              "           ...,\n",
              "           [250, 250, 246],\n",
              "           [250, 250, 246],\n",
              "           [249, 250, 246]],\n",
              "  \n",
              "          [[250, 251, 245],\n",
              "           [248, 249, 238],\n",
              "           [247, 247, 234],\n",
              "           ...,\n",
              "           [251, 251, 242],\n",
              "           [251, 252, 243],\n",
              "           [250, 251, 243]],\n",
              "  \n",
              "          [[251, 251, 244],\n",
              "           [250, 248, 237],\n",
              "           [250, 245, 233],\n",
              "           ...,\n",
              "           [250, 249, 238],\n",
              "           [250, 249, 240],\n",
              "           [250, 249, 242]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[221, 213, 191],\n",
              "           [221, 206, 176],\n",
              "           [225, 207, 181],\n",
              "           ...,\n",
              "           [199, 176, 134],\n",
              "           [207, 193, 165],\n",
              "           [233, 229, 226]],\n",
              "  \n",
              "          [[225, 223, 204],\n",
              "           [227, 219, 196],\n",
              "           [229, 216, 200],\n",
              "           ...,\n",
              "           [204, 185, 151],\n",
              "           [212, 201, 180],\n",
              "           [234, 232, 228]],\n",
              "  \n",
              "          [[233, 233, 226],\n",
              "           [234, 232, 224],\n",
              "           [235, 230, 225],\n",
              "           ...,\n",
              "           [219, 209, 194],\n",
              "           [223, 216, 207],\n",
              "           [232, 230, 228]]],\n",
              "  \n",
              "  \n",
              "         ...,\n",
              "  \n",
              "  \n",
              "         [[[248, 244, 242],\n",
              "           [240, 232, 223],\n",
              "           [236, 232, 223],\n",
              "           ...,\n",
              "           [233, 229, 222],\n",
              "           [230, 228, 222],\n",
              "           [238, 237, 233]],\n",
              "  \n",
              "          [[225, 213, 204],\n",
              "           [186, 167, 149],\n",
              "           [175, 159, 140],\n",
              "           ...,\n",
              "           [163, 148, 134],\n",
              "           [156, 144, 133],\n",
              "           [192, 184, 176]],\n",
              "  \n",
              "          [[209, 194, 179],\n",
              "           [144, 120,  95],\n",
              "           [139, 115,  87],\n",
              "           ...,\n",
              "           [109,  86,  67],\n",
              "           [109,  90,  76],\n",
              "           [157, 145, 135]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[161, 159, 153],\n",
              "           [ 39,  34,  28],\n",
              "           [ 28,  20,  14],\n",
              "           ...,\n",
              "           [ 93,  72,  53],\n",
              "           [ 85,  67,  50],\n",
              "           [136, 126, 115]],\n",
              "  \n",
              "          [[181, 179, 172],\n",
              "           [ 86,  83,  77],\n",
              "           [ 71,  68,  62],\n",
              "           ...,\n",
              "           [122, 103,  89],\n",
              "           [105,  92,  82],\n",
              "           [151, 145, 141]],\n",
              "  \n",
              "          [[224, 223, 218],\n",
              "           [180, 180, 175],\n",
              "           [173, 172, 167],\n",
              "           ...,\n",
              "           [196, 187, 180],\n",
              "           [183, 178, 174],\n",
              "           [204, 205, 205]]],\n",
              "  \n",
              "  \n",
              "         [[[156, 154, 137],\n",
              "           [151, 146, 123],\n",
              "           [151, 144, 125],\n",
              "           ...,\n",
              "           [155, 150, 129],\n",
              "           [152, 148, 125],\n",
              "           [186, 184, 163]],\n",
              "  \n",
              "          [[110, 106,  77],\n",
              "           [116, 108,  62],\n",
              "           [114, 101,  57],\n",
              "           ...,\n",
              "           [116, 106,  61],\n",
              "           [111, 103,  56],\n",
              "           [134, 129,  92]],\n",
              "  \n",
              "          [[116, 112,  82],\n",
              "           [124, 118,  66],\n",
              "           [128, 118,  67],\n",
              "           ...,\n",
              "           [ 99,  84,  43],\n",
              "           [101,  87,  43],\n",
              "           [129, 118,  86]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[109, 101,  74],\n",
              "           [112, 100,  54],\n",
              "           [118, 105,  62],\n",
              "           ...,\n",
              "           [126, 113,  65],\n",
              "           [126, 111,  61],\n",
              "           [138, 124,  89]],\n",
              "  \n",
              "          [[ 98,  92,  63],\n",
              "           [ 93,  82,  35],\n",
              "           [ 96,  83,  38],\n",
              "           ...,\n",
              "           [112,  96,  47],\n",
              "           [109,  92,  45],\n",
              "           [127, 113,  80]],\n",
              "  \n",
              "          [[170, 167, 145],\n",
              "           [160, 153, 118],\n",
              "           [163, 152, 119],\n",
              "           ...,\n",
              "           [161, 151, 114],\n",
              "           [156, 144, 107],\n",
              "           [163, 154, 126]]],\n",
              "  \n",
              "  \n",
              "         [[[ 31,  67, 122],\n",
              "           [ 30,  68, 124],\n",
              "           [ 31,  69, 126],\n",
              "           ...,\n",
              "           [ 32,  70, 129],\n",
              "           [ 32,  70, 125],\n",
              "           [ 32,  69, 122]],\n",
              "  \n",
              "          [[ 29,  68, 126],\n",
              "           [ 28,  69, 128],\n",
              "           [ 30,  69, 130],\n",
              "           ...,\n",
              "           [ 32,  70, 131],\n",
              "           [ 32,  69, 127],\n",
              "           [ 31,  69, 124]],\n",
              "  \n",
              "          [[ 30,  67, 126],\n",
              "           [ 29,  68, 128],\n",
              "           [ 30,  69, 130],\n",
              "           ...,\n",
              "           [ 32,  72, 132],\n",
              "           [ 31,  70, 130],\n",
              "           [ 30,  69, 127]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 39,  41,  76],\n",
              "           [ 38,  42,  76],\n",
              "           [ 38,  44,  78],\n",
              "           ...,\n",
              "           [ 39,  44,  79],\n",
              "           [ 38,  42,  77],\n",
              "           [ 39,  41,  76]],\n",
              "  \n",
              "          [[ 40,  39,  73],\n",
              "           [ 39,  40,  74],\n",
              "           [ 39,  41,  76],\n",
              "           ...,\n",
              "           [ 39,  41,  76],\n",
              "           [ 40,  41,  74],\n",
              "           [ 40,  39,  73]],\n",
              "  \n",
              "          [[ 40,  39,  70],\n",
              "           [ 40,  39,  71],\n",
              "           [ 40,  39,  72],\n",
              "           ...,\n",
              "           [ 41,  38,  72],\n",
              "           [ 39,  38,  69],\n",
              "           [ 40,  37,  67]]]], dtype=uint8), array([[19],\n",
              "         [29],\n",
              "         [ 0],\n",
              "         ...,\n",
              "         [ 3],\n",
              "         [ 7],\n",
              "         [73]])), (array([[[[199, 215, 249],\n",
              "           [196, 211, 244],\n",
              "           [195, 210, 243],\n",
              "           ...,\n",
              "           [216, 231, 250],\n",
              "           [217, 231, 250],\n",
              "           [224, 234, 252]],\n",
              "  \n",
              "          [[197, 210, 239],\n",
              "           [195, 208, 238],\n",
              "           [195, 210, 240],\n",
              "           ...,\n",
              "           [231, 243, 250],\n",
              "           [233, 243, 250],\n",
              "           [241, 245, 253]],\n",
              "  \n",
              "          [[222, 226, 246],\n",
              "           [213, 220, 242],\n",
              "           [209, 219, 243],\n",
              "           ...,\n",
              "           [243, 250, 251],\n",
              "           [244, 249, 251],\n",
              "           [250, 250, 253]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 72,  73,  99],\n",
              "           [ 71,  74, 102],\n",
              "           [ 74,  78, 108],\n",
              "           ...,\n",
              "           [220, 208, 217],\n",
              "           [183, 168, 181],\n",
              "           [155, 141, 150]],\n",
              "  \n",
              "          [[ 72,  75, 104],\n",
              "           [ 76,  81, 111],\n",
              "           [ 84,  89, 122],\n",
              "           ...,\n",
              "           [222, 212, 220],\n",
              "           [187, 174, 192],\n",
              "           [145, 132, 149]],\n",
              "  \n",
              "          [[ 80,  85, 118],\n",
              "           [ 84,  90, 123],\n",
              "           [ 85,  92, 127],\n",
              "           ...,\n",
              "           [217, 207, 215],\n",
              "           [207, 194, 211],\n",
              "           [176, 164, 183]]],\n",
              "  \n",
              "  \n",
              "         [[[113, 130,  98],\n",
              "           [ 88, 105,  73],\n",
              "           [ 72,  89,  58],\n",
              "           ...,\n",
              "           [105, 124,  93],\n",
              "           [ 86, 106,  74],\n",
              "           [ 63,  82,  55]],\n",
              "  \n",
              "          [[ 95, 113,  80],\n",
              "           [ 78,  97,  64],\n",
              "           [ 62,  80,  49],\n",
              "           ...,\n",
              "           [103, 122,  84],\n",
              "           [ 89, 109,  70],\n",
              "           [ 65,  83,  49]],\n",
              "  \n",
              "          [[ 79,  97,  62],\n",
              "           [ 65,  82,  49],\n",
              "           [ 52,  67,  37],\n",
              "           ...,\n",
              "           [ 65,  79,  52],\n",
              "           [ 62,  79,  51],\n",
              "           [ 49,  64,  38]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[121, 102,  86],\n",
              "           [127, 108,  96],\n",
              "           [139, 121, 111],\n",
              "           ...,\n",
              "           [ 80, 100,  71],\n",
              "           [ 75,  95,  68],\n",
              "           [ 66,  85,  62]],\n",
              "  \n",
              "          [[170, 145, 131],\n",
              "           [167, 145, 132],\n",
              "           [159, 140, 128],\n",
              "           ...,\n",
              "           [100, 113,  84],\n",
              "           [ 89, 103,  77],\n",
              "           [ 73,  87,  64]],\n",
              "  \n",
              "          [[168, 150, 135],\n",
              "           [170, 152, 137],\n",
              "           [173, 156, 141],\n",
              "           ...,\n",
              "           [118, 123,  96],\n",
              "           [ 99, 106,  80],\n",
              "           [ 84,  92,  67]]],\n",
              "  \n",
              "  \n",
              "         [[[ 61,  90, 120],\n",
              "           [ 61,  89, 119],\n",
              "           [ 67,  96, 125],\n",
              "           ...,\n",
              "           [ 70,  93, 124],\n",
              "           [ 64,  87, 118],\n",
              "           [ 64,  88, 119]],\n",
              "  \n",
              "          [[ 67,  97, 127],\n",
              "           [ 78, 108, 138],\n",
              "           [ 80, 109, 139],\n",
              "           ...,\n",
              "           [ 95, 113, 142],\n",
              "           [ 83, 101, 129],\n",
              "           [ 83, 102, 130]],\n",
              "  \n",
              "          [[ 64,  93, 123],\n",
              "           [ 66,  95, 125],\n",
              "           [ 61,  90, 120],\n",
              "           ...,\n",
              "           [120, 134, 160],\n",
              "           [109, 123, 150],\n",
              "           [112, 126, 152]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 84,  87,  87],\n",
              "           [ 67,  59,  38],\n",
              "           [ 84,  78,  33],\n",
              "           ...,\n",
              "           [204, 167, 160],\n",
              "           [235, 202, 192],\n",
              "           [236, 206, 193]],\n",
              "  \n",
              "          [[ 72,  64,  54],\n",
              "           [ 70,  54,  30],\n",
              "           [ 84,  79,  40],\n",
              "           ...,\n",
              "           [199, 157, 152],\n",
              "           [190, 150, 142],\n",
              "           [196, 161, 149]],\n",
              "  \n",
              "          [[ 72,  61,  47],\n",
              "           [ 76,  64,  35],\n",
              "           [ 77,  74,  37],\n",
              "           ...,\n",
              "           [227, 191, 189],\n",
              "           [155, 119, 115],\n",
              "           [134,  98,  89]]],\n",
              "  \n",
              "  \n",
              "         ...,\n",
              "  \n",
              "  \n",
              "         [[[ 24,  38,  31],\n",
              "           [ 24,  40,  31],\n",
              "           [ 22,  34,  28],\n",
              "           ...,\n",
              "           [ 56,  59,  55],\n",
              "           [ 35,  40,  37],\n",
              "           [ 25,  35,  32]],\n",
              "  \n",
              "          [[ 19,  29,  26],\n",
              "           [ 28,  45,  35],\n",
              "           [ 25,  40,  29],\n",
              "           ...,\n",
              "           [ 55,  57,  58],\n",
              "           [ 38,  42,  44],\n",
              "           [ 36,  44,  39]],\n",
              "  \n",
              "          [[ 23,  38,  28],\n",
              "           [ 31,  51,  35],\n",
              "           [ 29,  50,  32],\n",
              "           ...,\n",
              "           [ 33,  41,  36],\n",
              "           [ 33,  37,  41],\n",
              "           [ 42,  49,  44]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 39,  73,  29],\n",
              "           [ 47,  95,  25],\n",
              "           [ 61, 104,  44],\n",
              "           ...,\n",
              "           [ 41,  79,  35],\n",
              "           [ 71, 103,  72],\n",
              "           [ 54,  93,  53]],\n",
              "  \n",
              "          [[ 39,  72,  34],\n",
              "           [ 40,  91,  18],\n",
              "           [ 51,  96,  33],\n",
              "           ...,\n",
              "           [ 33,  70,  24],\n",
              "           [ 67, 100,  62],\n",
              "           [ 61, 101,  55]],\n",
              "  \n",
              "          [[ 48,  89,  33],\n",
              "           [ 37,  83,  17],\n",
              "           [ 46,  91,  24],\n",
              "           ...,\n",
              "           [ 33,  66,  20],\n",
              "           [ 65,  95,  54],\n",
              "           [ 60,  94,  54]]],\n",
              "  \n",
              "  \n",
              "         [[[ 86,  90,  80],\n",
              "           [ 93,  90,  75],\n",
              "           [ 28,  25,  21],\n",
              "           ...,\n",
              "           [ 73,  52,  36],\n",
              "           [ 67,  49,  39],\n",
              "           [ 92,  71,  61]],\n",
              "  \n",
              "          [[ 82,  86,  80],\n",
              "           [ 43,  44,  41],\n",
              "           [ 17,  17,  18],\n",
              "           ...,\n",
              "           [ 65,  42,  28],\n",
              "           [ 34,  23,  16],\n",
              "           [ 56,  41,  33]],\n",
              "  \n",
              "          [[ 64,  66,  60],\n",
              "           [ 25,  27,  26],\n",
              "           [ 39,  39,  39],\n",
              "           ...,\n",
              "           [ 54,  37,  24],\n",
              "           [ 34,  25,  19],\n",
              "           [ 40,  31,  22]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 43,  43,  39],\n",
              "           [ 87,  82,  70],\n",
              "           [112, 105,  91],\n",
              "           ...,\n",
              "           [130, 130, 123],\n",
              "           [128, 126, 121],\n",
              "           [119, 121, 119]],\n",
              "  \n",
              "          [[ 44,  42,  35],\n",
              "           [ 79,  74,  62],\n",
              "           [ 99,  95,  82],\n",
              "           ...,\n",
              "           [135, 133, 123],\n",
              "           [130, 127, 119],\n",
              "           [121, 120, 114]],\n",
              "  \n",
              "          [[ 85,  82,  70],\n",
              "           [ 97,  91,  77],\n",
              "           [ 90,  88,  76],\n",
              "           ...,\n",
              "           [132, 128, 119],\n",
              "           [129, 124, 115],\n",
              "           [120, 117, 109]]],\n",
              "  \n",
              "  \n",
              "         [[[246, 246, 242],\n",
              "           [240, 238, 232],\n",
              "           [214, 212, 199],\n",
              "           ...,\n",
              "           [ 74,  32,  35],\n",
              "           [ 77,  34,  37],\n",
              "           [ 81,  34,  35]],\n",
              "  \n",
              "          [[210, 205, 196],\n",
              "           [243, 240, 230],\n",
              "           [229, 225, 214],\n",
              "           ...,\n",
              "           [ 75,  33,  35],\n",
              "           [ 79,  35,  38],\n",
              "           [ 83,  34,  36]],\n",
              "  \n",
              "          [[144, 134, 112],\n",
              "           [175, 163, 144],\n",
              "           [158, 144, 130],\n",
              "           ...,\n",
              "           [ 74,  33,  35],\n",
              "           [ 79,  35,  38],\n",
              "           [ 82,  33,  36]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[198, 190, 176],\n",
              "           [111, 111,  66],\n",
              "           [ 58,  55,  27],\n",
              "           ...,\n",
              "           [ 62,  81,  41],\n",
              "           [ 72, 100,  41],\n",
              "           [ 80, 107,  49]],\n",
              "  \n",
              "          [[167, 160, 144],\n",
              "           [ 62,  64,  27],\n",
              "           [ 85,  85,  68],\n",
              "           ...,\n",
              "           [ 92, 126,  58],\n",
              "           [143, 183, 104],\n",
              "           [160, 199, 118]],\n",
              "  \n",
              "          [[115, 108,  94],\n",
              "           [ 42,  37,  21],\n",
              "           [139, 136, 127],\n",
              "           ...,\n",
              "           [139, 172, 114],\n",
              "           [167, 204, 141],\n",
              "           [146, 182, 118]]]], dtype=uint8), array([[49],\n",
              "         [33],\n",
              "         [72],\n",
              "         ...,\n",
              "         [51],\n",
              "         [42],\n",
              "         [70]])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGf6Tgtz0VTe"
      },
      "source": [
        "# Preprocess data function\n",
        "# Now that the data is loaded, we are going to build a preprocess function for the data. \n",
        "# We have X as a numpy array of shape (m, 32, 32, 3) where m is the number of images, \n",
        "# 32 and 32 the dimensions, and 3 is because we use color images (RGB). \n",
        "# We have a set of X for training and a set of X for validation. \n",
        "# Y is a numpy array of shape (m, ) that we want to be our labels. \n",
        "# Since we work with 10 different categories, we make use of one-hot encoding with a \n",
        "# function of Keras that makes our Y into a shape of (m, 10). That also applies for the validation.\n",
        "\n",
        "def preprocess_data(X,Y):\n",
        "  X_p = K.applications.xception.preprocess_input(X)\n",
        "  Y_p = K.utils.to_categorical(Y,100)\n",
        "  return X_p, Y_p"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4qZ9aHTEQUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffb2a23-8eb2-457f-c2d8-15eb786f2e3b"
      },
      "source": [
        "# load and split data\n",
        "# The data, split between train and test sets:\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = K.datasets.cifar100.load_data()\n",
        "img_rows, img_cols = 32, 32\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAs88uCaFMc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271899a5-10a2-4d87-bab9-1f52697a0402"
      },
      "source": [
        "# Preprocess data\n",
        "# Next, we are going to call our function with the parameters loaded from the Fashion Mnist database.\n",
        "\n",
        "x_train, y_train = preprocess_data(x_train, y_train)\n",
        "x_test, y_test = preprocess_data(x_test, y_test)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 100)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDEpJYeWGK46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02473eb6-bf56-42bf-83b1-aa041dbad3d9"
      },
      "source": [
        "# Using weights of a trained neural network\n",
        "# A pretrained model from the Keras Applications has the advantage of allowing you to use weights that\n",
        "# are already calibrated to make predictions. In this case, we use the weights from Imagenet \n",
        "# and the network is a Xception. The option include_top=False allows feature extraction by removing \n",
        "# the last dense layers. This let us control the output and input of the model.\n",
        "\n",
        "input_t = K.Input(shape=(32,32,3))\n",
        "Xception_Model = K.applications.Xception(include_top=False,\n",
        "                                    weights=\"imagenet\",\n",
        "                                    input_tensor=input_t)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlyKjCtPH3m3"
      },
      "source": [
        "# In this case, we ‘freeze’ all layers except for the last block of the Xception.\n",
        "\n",
        "for layer in Xception_Model.layers[:]:\n",
        "  layer.trainable=False\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdixTKJ7I10V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35abbda2-d3c8-4962-f85b-a33e27211d6d"
      },
      "source": [
        "# We can check that we did it correctly with:\n",
        "# False means that the layer is ‘freezed’ or is not trainable and \n",
        "# True that when we run our model, the weights are going to be adjusted.\n",
        "\n",
        "for i, layer in enumerate(Xception_Model.layers):\n",
        "  print(i,layer.name,\"-\",layer.trainable)\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1 - False\n",
            "1 block1_conv1 - False\n",
            "2 block1_conv1_bn - False\n",
            "3 block1_conv1_act - False\n",
            "4 block1_conv2 - False\n",
            "5 block1_conv2_bn - False\n",
            "6 block1_conv2_act - False\n",
            "7 block2_sepconv1 - False\n",
            "8 block2_sepconv1_bn - False\n",
            "9 block2_sepconv2_act - False\n",
            "10 block2_sepconv2 - False\n",
            "11 block2_sepconv2_bn - False\n",
            "12 conv2d - False\n",
            "13 block2_pool - False\n",
            "14 batch_normalization - False\n",
            "15 add - False\n",
            "16 block3_sepconv1_act - False\n",
            "17 block3_sepconv1 - False\n",
            "18 block3_sepconv1_bn - False\n",
            "19 block3_sepconv2_act - False\n",
            "20 block3_sepconv2 - False\n",
            "21 block3_sepconv2_bn - False\n",
            "22 conv2d_1 - False\n",
            "23 block3_pool - False\n",
            "24 batch_normalization_1 - False\n",
            "25 add_1 - False\n",
            "26 block4_sepconv1_act - False\n",
            "27 block4_sepconv1 - False\n",
            "28 block4_sepconv1_bn - False\n",
            "29 block4_sepconv2_act - False\n",
            "30 block4_sepconv2 - False\n",
            "31 block4_sepconv2_bn - False\n",
            "32 conv2d_2 - False\n",
            "33 block4_pool - False\n",
            "34 batch_normalization_2 - False\n",
            "35 add_2 - False\n",
            "36 block5_sepconv1_act - False\n",
            "37 block5_sepconv1 - False\n",
            "38 block5_sepconv1_bn - False\n",
            "39 block5_sepconv2_act - False\n",
            "40 block5_sepconv2 - False\n",
            "41 block5_sepconv2_bn - False\n",
            "42 block5_sepconv3_act - False\n",
            "43 block5_sepconv3 - False\n",
            "44 block5_sepconv3_bn - False\n",
            "45 add_3 - False\n",
            "46 block6_sepconv1_act - False\n",
            "47 block6_sepconv1 - False\n",
            "48 block6_sepconv1_bn - False\n",
            "49 block6_sepconv2_act - False\n",
            "50 block6_sepconv2 - False\n",
            "51 block6_sepconv2_bn - False\n",
            "52 block6_sepconv3_act - False\n",
            "53 block6_sepconv3 - False\n",
            "54 block6_sepconv3_bn - False\n",
            "55 add_4 - False\n",
            "56 block7_sepconv1_act - False\n",
            "57 block7_sepconv1 - False\n",
            "58 block7_sepconv1_bn - False\n",
            "59 block7_sepconv2_act - False\n",
            "60 block7_sepconv2 - False\n",
            "61 block7_sepconv2_bn - False\n",
            "62 block7_sepconv3_act - False\n",
            "63 block7_sepconv3 - False\n",
            "64 block7_sepconv3_bn - False\n",
            "65 add_5 - False\n",
            "66 block8_sepconv1_act - False\n",
            "67 block8_sepconv1 - False\n",
            "68 block8_sepconv1_bn - False\n",
            "69 block8_sepconv2_act - False\n",
            "70 block8_sepconv2 - False\n",
            "71 block8_sepconv2_bn - False\n",
            "72 block8_sepconv3_act - False\n",
            "73 block8_sepconv3 - False\n",
            "74 block8_sepconv3_bn - False\n",
            "75 add_6 - False\n",
            "76 block9_sepconv1_act - False\n",
            "77 block9_sepconv1 - False\n",
            "78 block9_sepconv1_bn - False\n",
            "79 block9_sepconv2_act - False\n",
            "80 block9_sepconv2 - False\n",
            "81 block9_sepconv2_bn - False\n",
            "82 block9_sepconv3_act - False\n",
            "83 block9_sepconv3 - False\n",
            "84 block9_sepconv3_bn - False\n",
            "85 add_7 - False\n",
            "86 block10_sepconv1_act - False\n",
            "87 block10_sepconv1 - False\n",
            "88 block10_sepconv1_bn - False\n",
            "89 block10_sepconv2_act - False\n",
            "90 block10_sepconv2 - False\n",
            "91 block10_sepconv2_bn - False\n",
            "92 block10_sepconv3_act - False\n",
            "93 block10_sepconv3 - False\n",
            "94 block10_sepconv3_bn - False\n",
            "95 add_8 - False\n",
            "96 block11_sepconv1_act - False\n",
            "97 block11_sepconv1 - False\n",
            "98 block11_sepconv1_bn - False\n",
            "99 block11_sepconv2_act - False\n",
            "100 block11_sepconv2 - False\n",
            "101 block11_sepconv2_bn - False\n",
            "102 block11_sepconv3_act - False\n",
            "103 block11_sepconv3 - False\n",
            "104 block11_sepconv3_bn - False\n",
            "105 add_9 - False\n",
            "106 block12_sepconv1_act - False\n",
            "107 block12_sepconv1 - False\n",
            "108 block12_sepconv1_bn - False\n",
            "109 block12_sepconv2_act - False\n",
            "110 block12_sepconv2 - False\n",
            "111 block12_sepconv2_bn - False\n",
            "112 block12_sepconv3_act - False\n",
            "113 block12_sepconv3 - False\n",
            "114 block12_sepconv3_bn - False\n",
            "115 add_10 - False\n",
            "116 block13_sepconv1_act - False\n",
            "117 block13_sepconv1 - False\n",
            "118 block13_sepconv1_bn - False\n",
            "119 block13_sepconv2_act - False\n",
            "120 block13_sepconv2 - False\n",
            "121 block13_sepconv2_bn - False\n",
            "122 conv2d_3 - False\n",
            "123 block13_pool - False\n",
            "124 batch_normalization_3 - False\n",
            "125 add_11 - False\n",
            "126 block14_sepconv1 - False\n",
            "127 block14_sepconv1_bn - False\n",
            "128 block14_sepconv1_act - False\n",
            "129 block14_sepconv2 - False\n",
            "130 block14_sepconv2_bn - False\n",
            "131 block14_sepconv2_act - False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PHC3U6xJceX"
      },
      "source": [
        "    # Add Flatten and Dense layers on top of Xception\n",
        "    # Now, we need to connect our pretrained model with the new layers \n",
        "    # of our model. We can use global pooling or a flatten layer to connect \n",
        "    # the dimensions of the previous layers with the new layers. \n",
        "    \n",
        "    to_res = (224, 224)\n",
        "    model = K.models.Sequential()\n",
        "    model.add(K.layers.Lambda(lambda image: tf.image.resize(image, to_res))) \n",
        "    model.add(Xception_Model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.Dense(256, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.5))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(128, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.5))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "   # model.add(K.layers.Dense(64, activation='relu'))\n",
        "   # model.add(K.layers.Dropout(0.5))\n",
        "   # model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(100, activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NClKTTPoJwjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024e593f-9637-4f8b-be92-ee92888a0f1b"
      },
      "source": [
        "# Compile model and train\n",
        "# Results\n",
        "# We obtained an accuracy of 94% on training set and 90% on validation with 10 epochs.\n",
        "# In the 8th epoch, the values are very similar and it is interesting to note that \n",
        "# in the first validation accuracy is higher than training. \n",
        "# This is because of dropout use, which in Keras, it has a different behavior \n",
        "# for training and testing. In testing time, all the features are ready and \n",
        "# the dropout is turned off, resulting in a better accuracy. \n",
        "# This readjust on the last epochs since the model continues changing on the training.\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='Adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=30, epochs=6, verbose=1,\n",
        "                        validation_data=(x_test, y_test)\n",
        "                       )\n",
        "model.summary()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/6\n",
            "50000/50000 [==============================] - 332s 7ms/sample - loss: 5.0476 - acc: 0.0126 - val_loss: 4.5516 - val_acc: 0.0288\n",
            "Epoch 2/6\n",
            "50000/50000 [==============================] - 321s 6ms/sample - loss: 4.8808 - acc: 0.0187 - val_loss: 4.3532 - val_acc: 0.0603\n",
            "Epoch 3/6\n",
            "50000/50000 [==============================] - 321s 6ms/sample - loss: 4.7564 - acc: 0.0276 - val_loss: 4.1883 - val_acc: 0.1007\n",
            "Epoch 4/6\n",
            "50000/50000 [==============================] - 322s 6ms/sample - loss: 4.6430 - acc: 0.0349 - val_loss: 4.0385 - val_acc: 0.1458\n",
            "Epoch 5/6\n",
            "50000/50000 [==============================] - 321s 6ms/sample - loss: 4.5539 - acc: 0.0438 - val_loss: 3.9162 - val_acc: 0.1836\n",
            "Epoch 6/6\n",
            "50000/50000 [==============================] - 321s 6ms/sample - loss: 4.4617 - acc: 0.0526 - val_loss: 3.8002 - val_acc: 0.2149\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              multiple                  0         \n",
            "_________________________________________________________________\n",
            "xception (Model)             (None, 1, 1, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  25690368  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch multiple                  1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch multiple                  512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  12900     \n",
            "=================================================================\n",
            "Total params: 46,599,180\n",
            "Trainable params: 25,736,932\n",
            "Non-trainable params: 20,862,248\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc9XmTGrA7uE",
        "outputId": "0b3554aa-339e-4a3b-b3f5-6f7c51398adc"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 54s 5ms/sample - loss: 3.8002 - acc: 0.2149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.800191848373413, 0.2149]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}